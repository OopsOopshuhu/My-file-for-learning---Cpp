## 操作系统知识整理
* 操作系统的四个特征：并发，共享，虚拟，异步
![](2023-03-03-09-16-35.png)
* 用户态->核心态 中断！
  核心态->用户态 PSW置位
* 内中断---异常，例外，**陷入** (缺页，整数/0，系统调用)
  > 系统调用是操作系统给用户的接口，相关处理在核心态，库函数一般封装了系统调用的复杂细节
* 外中断处理过程---IO中断，人工干预
  1. 执行完每个指令，CPU都要检查是否有外部中断信号
  2. 检查到，保护中断进程的CPU环境，PSW，计数器PC，通用寄存器
  3. 根据中断类型转入中断处理程序
  4. 恢复原进程CPU环境，关中断
---
#### 1. 进程
* 进程实体
  1. PCB---进程的唯一标识
  2. 程序段
  3. 数据段
* 进程控制
  ![](2023-03-03-09-33-48.png)
  > 进程状态转换---原语执行。图上未显示的还有就绪队列和阻塞队列，CPU是一个个处理的
* 进程通信
  * 共享存储---互斥访问
  * 消息传递---消息挂到接收进程消息的缓冲队列，或者是管理一个信箱
  * 管道通信---开辟在内存的缓冲区
#### 2. 线程
* 线程---CPU执行流的最小单位，增加并发性。
  * 一个进程可以有多个线程。CPU多核下，线程可以占用不同CPU。
  * 每个线程都有id和TCB
  * 线程也有就绪阻塞运行态
  * 线程几乎不拥有系统资源
  * 切换进程系统开销大
* 用户级/内核级线程---通过线程库连接
  用户这个进程有三个线程，该进程即使运行在4核处理机上，也只能分配到两个核并行，因为处理机的分配单位是内核级线程！！！
---
* 进程线程区别
  1. 线程共享进程的地址空间，但有自己独立的stack，向栈中压数据超过容量会发生页错误
   > ulimit-s 设置栈堆大值，超过报 stack overflow, 收到 segment fault, 调高栈容增加内存开销和启动时间
  2. 进程挂了不影响其他进程，线程挂了影响整个进程，多进程比多线程健壮
  3. 进程切换开销大
  4. 都可以并发
  5. 线程不能独立运行
* 处理机调度
  * 高级调度(作业调度)---从外存队列调度，分配内存和资源，建立进程PCB，获取争取CPU的权力
  * 中级调度(内存调度)---引入虚拟内存技术后，将暂时不能运行的进程调出内存(挂起态)，提高内存利用率和系统吞吐量。也决定哪个挂起进程重新调入内存
  > 挂起进程的PCB被放到挂起队列
  * 低级调度(进程调度)---选一个进程分配处理机
  ![](2023-03-03-09-53-54.png)
* 不能进行进程交换和调度的情况
  1. CPU处理中断过程中---如果可以的话就别想干活了，处理一半你给叉出去了
  2. 进程在内核程序临界区---访问某种内核数据结构，比如说PCB队列，会上锁
  3. 原语中
* 信号量机制(dijskra)---有点像条件变量，实现进程互斥和同步
* 产生死锁的条件
  1. 竞争互斥使用的资源
  2. 非抢占
  3. 请求保持
  4. 循环等待
* 避免死锁---银行家算法，安全序列，或者破坏上面任何一个条件(死锁预防)
---
#### 3. 内存
* 写程序到运行的过程
  1. 预处理---头文件插入程序文本 .i结尾
  2. 编译---将 .i 文件翻译成文本文件 .s 。每条语句都以标准文本格式确切描述一条低级机器语言指令
  3. 汇编---.s 翻译成机器语言指令，打包成可重定位目标程序 .o 
  4. 链接---合并动静态库函数到 .o 中，得到可执行文件
  5. 运行装入---模块装内存
* 内存的非连续分配管理---分页存储，分段存储
  * 分页存储---页面丢进页框，动态重定位进行地址转换(模块的起始地址+页内偏移量)
  > 虚拟存储技术---缺页中断再调入内存
  * 分段存储---每段有段号，段1 main函数，段2 子函数， 段3 保存全局变量。CPU通过段号区分各段
* 分页分段管理对比
  1. 分页是系统行为，段是逻辑单位，更好满足用户需求，用户看得到
  2. 找业内进程地址给个地址即可，找段内进程地址，还要给出段号
  3. 分段可以实现信息共享和保护
  > 操作系统一般是段页式管理，进程先分段，再分页，段表存放页表号
* 虚拟内存技术---基于局部性原理(时间/空间局部性)
  > 高速缓冲技术---将近期频繁访问的数据放到更高速的存储器，如高速缓存或者寄存器
  虚拟内存---程序装入时，用到的放内存，用不到的放外村，空间不够就往外扔
* 抖动(颠簸)---刚丢的页面又需要又不需要，主要是驻留集页框不够
  解决方式---工作集(滑动窗口)
### 面试用
#### 1. 进程创建过程，函数，数据结构
1. fork --- 创建父进程的完整副本，包括内存的task_struct内容
2. vfork --- 子父个共享数据段，子先于父运行
3. pthread库clone系统调用 --- linux创建线程
#### 2. fork
函数原型: pid_t fork() - 对于父进程返回子进程pid，子进程返0, 出错返-1
```cpp
int pid = fork();
if (pid < 0) {} //失败，进程数受限或者内存用光
else if (pid == 0){} //子进程执行代码
else {} //父进程执行代码
```
#### 3. 父子进程通信---pipe()和fork()
#### 4. 孤儿进程，僵尸进程，守护进程
* 孤儿进程---一般情况父先退会wait或者waitpid等子进程结束，如果不等，子进程被init(pid = 1)收养
* 僵尸进程---子先退了，父未结束且没有 wait或者waitpid 获取子的状态，子残留的状态 (task_struct结构和少量资源信息) 变成僵尸
* 守护进程---后台运行，独立于控制终端，周期性执行任务。脱离终端为了避免干扰其他进程以及被终端打断
  * 特点：
  1. 后台运行
  2. 与之前运行环境隔开
  3. 启动可以从脚本 /etc/rc.d ，可以由作业规划 crond 启动，可以shell运行
  * 实现
  1. 父域：fork 并 exit
  2. 子域：setsid---创建会话
  3. 子域：chdir---让跟目录“/”成为子进程工作目录
  4. 子域：umask---设置进程 umask 为0
  5. 子域关闭任何不需要的文件描述符
#### 5. 啥时候多线程，啥时候多进程
* 多线程---频繁创建销毁，大量计算，强相关处理，多核分布
* 多进程---弱相关处理，可能扩展到多机分布
#### 6. 协程---用户态线程，可暂停的函数，线程是用来运行函数的地方
![](./%E7%BA%BF%E7%A8%8B.jpg)
1. 比线程更轻量级，用户程序控制
2. 开销远小于线程
3. 有自己的寄存器上下文和栈。调度切换时，可以保存和恢复
4. 跨平台、跨体系架构、无需线程上下文切换开销、方便切换控制流，简化编程模型
5. 完成主要靠 yeild 关键字， 执行过程中，在子程序内部可中断，然后转而执行别的子程序，适当时候返回来继续
6. 执行效率高，线程数越多，性能优势越明显
7. 不要要多线程的锁
> 协程的目的---实现异步逻辑
> 做异步做并发，要开多线程，写函数回调，非阻塞代码。线程数一多，CPU飙高，可扩展性很差
#### 7. 递归锁
> 读写锁，共享版的互斥锁，降低线程互斥产生的代价
mutex 分为递归锁和非递归锁
递归锁---可重入锁
非递归锁---不可重入锁
区别：同一个线程可以多次获取同一个递归锁，不会死锁。但非递归锁会
#### 8. 执行一个系统调用，os发生的过程
1. 库函数(fork)
2. 根据 glibc 函数实现，拿到调用号并执行 int $0x80 产生中断信号
3. 地址空间转换和栈切换，执行 save_all 进内核模式
4. 中断处理，根据调用表调用内核函数
5. 执行内核函数
6. 执行 restore_all 返回用户模式
#### 9. 线程安全，如何实现
* 线程安全问题主要是由**全局变量**和**静态变量**引起的
如果有多个线程对变量进行写操作，一般都需要考虑线程同步，否则可能影响线程安全
* 安全实现方式
  * 加锁---synchronized 或者 reentrantlock ，实现线程执行的串行化
  * 非阻塞IO同步---冲突了就一直重试直到成功
  * 线程本地化---threadlocal 每一个线程创造一个共享变量的副本，就不会同时操作一个变量了
  * 为什么没有条件变量？信号量机制？
---
# 这是一个关于socket网络编成的学习文件
![](2023-02-17-10-41-59.png)
listen---会建立一个fd队列，当服务器在服务时来了请求，先放在队列中，每次循环从池子里捞一个。监听哪些fd
select/poll---要去遍历队列看哪个就绪
epoll---知道哪个有数据，直接拿

## 阻塞IO
单线程---会堵死
多线程---资源浪费，只有几个就绪的socket，并且线程调度，上下文切换，占用内存都是问题

## 非阻塞IO
用户态和内核态通过系统调用，如read，write等socketAPI，相互切换。
read每一次调用，都会从用户态的队列中复制一个fd到内核，内核处理。
如果有数据达到，返回一个合法的非负整数，如果没到达返回一个非法整数
如果是合法的，就会进行后续的逻辑处理
1. 主线程直接去处理这个fd
2. 创建一个新的线程去处理fd，主线程继续检查池子里的fd

>缺点：需要不断遍历进行系统调用(用户空间不断read)，有一定开销

##1. select
```cpp
# 获取就绪fd
@param nfds         #要监听所有fdset最大值+1，就是最大fd
@param readfds      #要监听可读fdset
@param writefds     #要监听可写fdset
@param exceptfds    #要监听异常fdset
@param timeval      #本次调用超时时间
@return             #大于0：

int select(int nfds, 
           fd_set *readfds, 
           fd_set *writefds, 
           fd_set *exceptfds, 
           struct timeval *timeout #>0---就等这么久，=0---不等待，-1---一直等
)
```
流程：
调用select---将用户空间的fd_set copy到内核
内核遍历fd_set，检查到就绪的fd，并打标记，继续遍历。最终返回就绪fd的数量
用户知道有事件就绪了，但并不知道是哪个fd上的socket就绪，所以再遍历fd_set
然后下一次的select调用

>如果内核fd_set没一个就绪的，阻塞当前的用户进程
1 当客户端向服务端发送数据时，数据通过网络传输达到服务端网卡
2 网卡通过DMA将数据放入指定内存
3 处理完之后通过中断信号告诉cpu有新的数据包到达
4 cpu收到中断信号响应中断
5 调用中断处理程序进行处理：
根据数据包的ip和端口号找到对应的这个socket，然后将数据保存到这个socket接受队列，再检查这个socket对应的等待队列里面是否有进程正在阻塞等待，如果有就唤醒进程，用户空间进程唤醒之后，内核继续检查一遍fd_set。如果检查到某些fd就绪，给fd打标记，结束阻塞。再返回给用户空间...

监听的fd是一个bitmap，哪些就绪了就重置哪些bit。bitmap默认1024
入参的3个fd_set每次调用都要重置成想要监听的fd_set
readfds* :
0|0|0|0|0|1|0|1
-|-|-|-|-|-|-|-
0|0|0|0|0|1|0|0
第一行：入参---哪些fd是想要监听的
第二行：回参---哪些fd是就绪的

##2. poll
```cpp
# 获取就绪fd
@param pollfd       #fdset
@param nfds         #fd数量
@param timeout      #本次调用的超时时间
@return             #大于0：就绪fd数量，等于0：超时， 小于0：出错

int poll(struct pollfd *fds, 
           unsigned int nfds, 
           int timeout);
            
           struct pollfd{ 
                int fd;//监听的fd
                short events; //监听的事件
                short revents; //就绪的事件
           }
```
>仅仅是听过event和revent，来分开存储监听的事件和就绪的事件。就不用像select那样每次调用完之后都要重置要监听的事件

>拷贝到内核之后其实是一个链表，用链表来存储要监听的fd，所以没有1024的限制

##3. epoll---基于 reactor 模型，多线程并发 
```cpp
# 创建一个epoll
@param size    #epoll要监听的fd数量
@return epoll  #epoll的fd

int epoll_create(int size); //创建了这个epoll对应的epfd,返回的是一个epfd,后面就根据这个epfd去操作epoll。即创建一个大小为要监听fd数量的epoll“容器”，并取名和返回这个epfd。这个“容器”里存放的都是fd

# 事件注册
@param epfd          #epoll的fd，epoll_create创建时返回
@param op            #操作类型：新增(1),删除(2),更新(3)
@param fd            #本次要操作的那些fd
@param epoll_event   #需要监听的事件：读事件、写事件等
@return              #调用成功返回0,否则返回-1

int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

# 获取就绪事件
@param epfd          #epoll的fd，epoll_create创建时返回
@param events        #用于回传就绪的事件
@param maxevents     #每次能处理的最大事件数
@param timeout       #等待IO事件发生的超时时间，-1阻塞，0非阻塞
@return              #大于0：就绪fd数量，等于0：超时， 小于0：出错

int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);//epoll_event用于回传一个就绪事件，就是调用内核，内核将事件写入epollevent
```
![](2023-03-04-09-49-46.png)
流程：
1. 调用**epoll_create**在内核创建一个epoll,epoll底层是**eventpoll**，eventpoll数据结构主要包含3个参数
   1. wq---等待队列
   >没有fd就绪，进程阻塞，阻塞的时候将进程关联到等待队列，后续可唤醒
   2. rdllist---就绪链表
   >就绪fd加到链表，可以很快知道哪些是就绪的
   3. rbr---红黑树
   >将想要监听的这些事件fd通过红黑树存储，通过红黑树进行高效的增删改查操作
2. 调用**epoll_ctl**，将一个fd copy到内核空间，并封装成**epitem**，epitem数据结构除了存储fd之外还有rbn,ffd,ep,pwqlist
   1. rbn---关联到红黑树
   2. ffd---关联到就绪链表
   3. ep---关联到eventpoll对象
   4. pwqlist---一个等待链表，关联到一个回调函数 ep_poll_callback(有事件进程唤醒)
3. 调用**epoll_ctl**继续向eventpoll的rbr去添加fd，也就是加入新的监听fd。
4. 最后调 **epoll_wait** 检查就绪链表，如果就绪链表为NULL，就添加到等待队列，进程让出cpu。如果这个时候client有数据过来，client->数据包->网卡->NMA->socket缓冲区->中断->socket->epitem,通过socket找到epitem里的回调函数ep_poll_callback，函数将fd添加到就绪链表，然后唤醒eventpoll中等待队列里面的进程，该进程接着判断rdllist就绪链表是否有就绪事件，没有的话就将这个事件直接返回用户空间，用户空间收到fd之后对事件进行处理。如果就绪链表已经有就绪事件，返回整个就绪链表，到用户空间转化成就绪数组，然后对事件进行处理

再一个高并发网络下，这个事件是很快的，所以每次epoll_wait调用的时候，就绪列表都是有事件的，所以每次epoll_wait调用之后都能够很快返回

缺点：
1. 跨平台性不好，只支持linux
2. 在监听连接数和事件较少的情况下，select可能更优
3. 基于事件驱动，回调函数不能阻塞---非抢占，可抢占+协程，提高并发度。
> 你来一个socket，我创建个线程，来一个创一个，很容易进程线程耗尽，也给系统带来负担，所以创建大量的进线程是不合理的。异步编程框架的回调函数分布在不同的执行单元，当多个异步操作需要顺序执行时，会产生多层嵌套的回调函数，难读且不易维护

> 就比如说，我要用异步网络请求获取数据，然后对数据处理，然后保存到数据库，回调地狱
1 handle方式发送异步网络请求
2 请求成功，对数据处理
3 handle方式链接数据库
4 链接成功，处理结果保存到数据库
5 保存成功，打印信息
## 水平触发和边缘触发
###1. LT： Level-triggered(默认)
epoll_wait 检测到事件后，如果该事件没被处理完毕，后续每次 epoll_wait 调用都会返回该事件，也就是说会不断把这个事件传到用户态一直处理

###2. ET: Edge_triggered
epoll_wait 检测到事件后，只会在当次返回该事件，不管该事件是否被处理完毕。

---
### 为什么要使用协程？
* 协程的目的在于剔除线程的阻塞，提高CPU利用率
* 减少线程的重复高频创建---常规解决办法：线程池
* Reactor && 非阻塞回调实现高并发和高性能的网络服务，解决问题的能力有限，响应式编程，容易陷入回调地狱，割裂业务逻辑---解决办法：协程
    * 当reactor 检测到一个IO可以开始时间，会通知相应的回调函数去执行操作，但是这个过程可能是阻塞的，回调函数上个任务还没做完，比如大量的读写事件
    * 需要我们自己编写回调函数处理不同事件，如果函数之间有依赖关系，会进行嵌套，容易进入回调地狱
    * reactor模式对于CPU密集型任务不太合适，只是利用单个线程或者进程来处理所有事件。如果某个任务需要大量计算或者阻塞其他资源(数据库)，拖慢整个系统的响应速度
    * 少使用回调函数，减少回调链深度，提升代码的可维护与可理解性，尽量避免回调地狱
* 协程可以用同步编程的方式实现异步编程代码，提高可读可为维护性
* N:1线程库，所有协程运行于一个线程，协程间切换不用系统调用，代价无法利用多核，代码必须非阻塞，不然会被卡住
* N:M线程库，协程的所有信息都保存在上下文（Contex）对象中，将不同上下文分发给不同的线程就可以实现协程的跨线程执行，如此，协程被阻塞的概率将减小。跨线程的协程之间不会影响
* 协程可以配合IO多路复用技术来实现高并发的网络服务，比如使用boot.asio库
### epoll和reactor区别
epoll---监听socket_fd，提供三个API处理fd和事件，内核级别的模块
reactor---基于事件分发的网络编程模式，处理多个客户端请求，应用程序级别的模式。封装底层IO操作转化为事件处理。两个主要组件：事件循环(监听和分发事件)，事件处理器(注册和执行回调)
> reactor是对epoll的封装，简化网络编程逻辑，提高代码可读性和可维护性
### 无栈协程和有栈协程(逻辑栈)
有栈：协程之间存在联系，一个在等一个完成---非对称协程
无栈：两个协程被调用的概率相同---对称协程
